{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaoVZSrv48aE"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "## Homework 4 - Regularization \n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KsLD_en48aI"
   },
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- **This homework must be completed individually.**\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "\n",
    "\n",
    "Names of people you have worked with goes here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KsLD_en48aI"
   },
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "O1guCDAx48aK",
    "outputId": "0aa1afea-ee13-46c0-cdf0-150d57fd1ad8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kh0dA7W48ad"
   },
   "source": [
    "import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j02kGLE-48ah"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "from pandas.core import datetools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on regularization and cross validation. We will continue to build regression models for the [Capital Bikeshare program](https://www.capitalbikeshare.com) in Washington D.C.  See homework 3 for more information about the Capital Bikeshare data that we'll be using extensively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 1 [20pts]  Data pre-processing </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNsROjOHaAA2"
   },
   "source": [
    "**1.1** Read in the provided `bikes_student.csv` to a data frame named `bikes_main`. Split it into a training set `bikes_train` and a validation set `bikes_val`. Use `random_state=90`, a test set size of .2, and stratify on month. Remember to specify the data's index column as you read it in.\n",
    "\n",
    "**1.2** As with last homework, the response will be the `counts` column and we'll drop `counts`, `registered` and `casual` for being trivial predictors, drop `workingday` and `month` for being multicollinear with other columns, and `dteday` for being inappropriate for regression. Write code to do this.\n",
    "\n",
    "Encapsulate this process as a function with appropriate inputs and outputs, and **test** your code by producing `practice_y_train` and `practice_X_train`.\n",
    "\n",
    "**1.3** Write a function to standardize a provided subset of columns in your training/validation/test sets. Remember that while you will be scaling all of your data, you must learn the scaling parameters (mean and SD) from only the training set.\n",
    "\n",
    "Test your code by building a list of all non-binary columns in your `practice_X_train` and scaling only those columns. Call the result `practice_X_train_scaled`. Display the `.describe()` and verify that you have correctly scaled all columns, including the polynomial columns.\n",
    "\n",
    "**Hint: employ the provided list of binary columns and use `pd.columns.difference()`**\n",
    "\n",
    "`binary_columns = [ 'holiday', 'workingday','Feb', 'Mar', 'Apr',\n",
    "       'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', 'spring',\n",
    "       'summer', 'fall', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat',\n",
    "       'Cloudy', 'Snow', 'Storm']`\n",
    "\n",
    "\n",
    "**1.4** Write a code to augment your a dataset with higher-order features for `temp`, `atemp`, `hum`,`windspeed`, and `hour`. You should include ONLY the pure powers of these columns. So with degree=2 you should produce `atemp^2` and `hum^2` but not `atemp*hum` or any other two-feature interactions. \n",
    "\n",
    "\n",
    "Encapsulate this process as a function with appropriate inputs and outputs, and test your code by producing `practice_X_train_poly`, a training dataset with quadratic and cubic features built from `practice_X_train_scaled`, and printing `practice_X_train_poly`'s column names and `.head()`.\n",
    "\n",
    "**1.5** Write code to add interaction terms to the model. Specifically, we want interactions between the continuous predictors (`temp`,`atemp`, `hum`,`windspeed`) and the month and weekday dummies (`Feb`, `Mar`...`Dec`, `Mon`, `Tue`, ... `Sat`). That means you SHOULD build `atemp*Feb` and `hum*Mon` and so on, but NOT `Feb*Mar` and NOT `Feb*Tue`. The interaction terms should always be a continuous feature times a month dummy or a continuous feature times a weekday dummy.\n",
    "\n",
    "\n",
    "Encapsulate this process as a function with appropriate inputs and outputs, and test your code by adding interaction terms to `practice_X_train_poly` and show its column names and `.head()`**\n",
    "\n",
    "**1.6** Combine all your code so far into a function that takes in `bikes_train`, `bikes_val`, the names of columns for polynomial, the target column, the columns to be dropped and produces computation-ready design matrices `X_train` and `X_val` and responses `y_train` and `y_val`. Your final function should build correct, scaled design matrices with the stated interaction terms and any polynomial degree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Read in the provided `bikes_student.csv` to a data frame named `bikes_main`. Split it into a training set `bikes_train` and a validation set `bikes_val`. Use `random_state=90`, a test set size of .2, and stratify on month. Remember to specify the data's index column as you read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWIlhxR39wwq"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "bikes_main=pd.read_csv('data/bikes_student.csv')\n",
    "bikes_main=bikes_main.drop('Unnamed: 0',axis=1)\n",
    "bikes_main.head()\n",
    "bikes_train,bikes_val=train_test_split(bikes_main,test_size=0.2,random_state=90,stratify=bikes_main['month'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0YncxWuaW3k"
   },
   "source": [
    "**1.2** As with last homework, the response will be the `counts` column and we'll drop `counts`, `registered` and `casual` for being trivial predictors, drop `workingday` and `month` for being multicolinear with other columns, and `dteday` for being inappropriate for regression. Write code to do this.\n",
    "\n",
    "Encapsulate this process as a function with appropriate inputs and outputs, and test your code by producing `practice_y_train` and `practice_X_train`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>Feb</th>\n",
       "      <th>Mar</th>\n",
       "      <th>Apr</th>\n",
       "      <th>...</th>\n",
       "      <th>fall</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Cloudy</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Storm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5152</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.2239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3030</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hour  year  holiday  temp   atemp   hum  windspeed  Feb  Mar  Apr  ...    \\\n",
       "540    23     1        0  0.54  0.5152  0.73     0.1045    0    0    0  ...     \n",
       "367    11     0        0  0.76  0.6667  0.35     0.2239    0    0    0  ...     \n",
       "869     2     1        0  0.66  0.6212  0.69     0.0000    0    0    0  ...     \n",
       "216     5     1        0  0.30  0.3030  0.81     0.1343    0    0    0  ...     \n",
       "238    21     1        0  0.70  0.6515  0.61     0.1642    0    0    0  ...     \n",
       "\n",
       "     fall  Mon  Tue  Wed  Thu  Fri  Sat  Cloudy  Snow  Storm  \n",
       "540     1    0    1    0    0    0    0       0     0      0  \n",
       "367     0    0    0    1    0    0    0       0     0      0  \n",
       "869     0    0    0    0    0    1    0       0     0      0  \n",
       "216     1    0    0    1    0    0    0       1     0      0  \n",
       "238     0    0    0    0    0    1    0       1     0      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "def drop_features(df,target_col,drop_col):\n",
    "    y_train=df[[target_col]]\n",
    "    df=df.drop([target_col],axis=1)\n",
    "    x_train=df.drop(drop_col,axis=1)\n",
    "    return x_train,y_train\n",
    "\n",
    "practice_X_train,practice_y_train=drop_features(bikes_train,'counts',['registered','casual','workingday','month','dteday'])\n",
    "practice_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mr3zjw-Y48cx"
   },
   "source": [
    "**1.3** Write a function to standardize a provided subset of columns in your training/validation/test sets. Remember that while you will be scaling all of your data, you must learn the scaling parameters (mean and SD) from only the training set.\n",
    "\n",
    "Test your code by building a list of all non-binary columns in your `practice_X_train` and scaling only those columns. Call the result `practice_X_train_scaled`. Display the `.describe()` and verify that you have correctly scaled all columns, including the polynomial columns.\n",
    "\n",
    "**Hint: employ the provided list of binary columns and use `pd.columns.difference()`**\n",
    "\n",
    "`binary_columns = [ 'holiday', 'workingday','Feb', 'Mar', 'Apr',\n",
    "       'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', 'spring',\n",
    "       'summer', 'fall', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat',\n",
    "       'Cloudy', 'Snow', 'Storm']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "pIm1TIqf48c4",
    "outputId": "af5391ee-8a4b-49da-ade4-4e8411fb412e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>Feb</th>\n",
       "      <th>Mar</th>\n",
       "      <th>Apr</th>\n",
       "      <th>...</th>\n",
       "      <th>fall</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Cloudy</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Storm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.994516e-16</td>\n",
       "      <td>2.686740e-17</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>3.019807e-17</td>\n",
       "      <td>-1.256772e-16</td>\n",
       "      <td>5.995204e-17</td>\n",
       "      <td>1.301181e-16</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.15000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>0.162164</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>0.268306</td>\n",
       "      <td>0.279021</td>\n",
       "      <td>0.274502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432068</td>\n",
       "      <td>0.350248</td>\n",
       "      <td>0.355278</td>\n",
       "      <td>0.368635</td>\n",
       "      <td>0.334257</td>\n",
       "      <td>0.33314</td>\n",
       "      <td>0.35725</td>\n",
       "      <td>0.449224</td>\n",
       "      <td>0.274502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.646163e+00</td>\n",
       "      <td>-1.018165e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.347976e+00</td>\n",
       "      <td>-2.402605e+00</td>\n",
       "      <td>-3.397602e+00</td>\n",
       "      <td>-1.554205e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.189949e-01</td>\n",
       "      <td>-1.018165e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.922693e-01</td>\n",
       "      <td>-8.121270e-01</td>\n",
       "      <td>-7.421467e-01</td>\n",
       "      <td>-7.231056e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.639332e-02</td>\n",
       "      <td>9.821591e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.744066e-02</td>\n",
       "      <td>7.147176e-02</td>\n",
       "      <td>5.448995e-02</td>\n",
       "      <td>-1.130295e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.262083e-01</td>\n",
       "      <td>9.821591e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.671507e-01</td>\n",
       "      <td>8.670022e-01</td>\n",
       "      <td>8.511266e-01</td>\n",
       "      <td>4.634972e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.698810e+00</td>\n",
       "      <td>9.821591e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.319143e+00</td>\n",
       "      <td>2.546131e+00</td>\n",
       "      <td>1.913309e+00</td>\n",
       "      <td>5.211499e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               hour          year      holiday          temp         atemp  \\\n",
       "count  1.000000e+03  1.000000e+03  1000.000000  1.000000e+03  1.000000e+03   \n",
       "mean  -1.994516e-16  2.686740e-17     0.027000  3.019807e-17 -1.256772e-16   \n",
       "std    1.000500e+00  1.000500e+00     0.162164  1.000500e+00  1.000500e+00   \n",
       "min   -1.646163e+00 -1.018165e+00     0.000000 -2.347976e+00 -2.402605e+00   \n",
       "25%   -9.189949e-01 -1.018165e+00     0.000000 -7.922693e-01 -8.121270e-01   \n",
       "50%   -4.639332e-02  9.821591e-01     0.000000  3.744066e-02  7.147176e-02   \n",
       "75%    8.262083e-01  9.821591e-01     0.000000  8.671507e-01  8.670022e-01   \n",
       "max    1.698810e+00  9.821591e-01     1.000000  2.319143e+00  2.546131e+00   \n",
       "\n",
       "                hum     windspeed          Feb          Mar          Apr  \\\n",
       "count  1.000000e+03  1.000000e+03  1000.000000  1000.000000  1000.000000   \n",
       "mean   5.995204e-17  1.301181e-16     0.078000     0.085000     0.082000   \n",
       "std    1.000500e+00  1.000500e+00     0.268306     0.279021     0.274502   \n",
       "min   -3.397602e+00 -1.554205e+00     0.000000     0.000000     0.000000   \n",
       "25%   -7.421467e-01 -7.231056e-01     0.000000     0.000000     0.000000   \n",
       "50%    5.448995e-02 -1.130295e-02     0.000000     0.000000     0.000000   \n",
       "75%    8.511266e-01  4.634972e-01     0.000000     0.000000     0.000000   \n",
       "max    1.913309e+00  5.211499e+00     1.000000     1.000000     1.000000   \n",
       "\n",
       "        ...           fall          Mon          Tue          Wed  \\\n",
       "count   ...    1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean    ...       0.248000     0.143000     0.148000     0.162000   \n",
       "std     ...       0.432068     0.350248     0.355278     0.368635   \n",
       "min     ...       0.000000     0.000000     0.000000     0.000000   \n",
       "25%     ...       0.000000     0.000000     0.000000     0.000000   \n",
       "50%     ...       0.000000     0.000000     0.000000     0.000000   \n",
       "75%     ...       0.000000     0.000000     0.000000     0.000000   \n",
       "max     ...       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               Thu         Fri         Sat       Cloudy         Snow   Storm  \n",
       "count  1000.000000  1000.00000  1000.00000  1000.000000  1000.000000  1000.0  \n",
       "mean      0.128000     0.12700     0.15000     0.280000     0.082000     0.0  \n",
       "std       0.334257     0.33314     0.35725     0.449224     0.274502     0.0  \n",
       "min       0.000000     0.00000     0.00000     0.000000     0.000000     0.0  \n",
       "25%       0.000000     0.00000     0.00000     0.000000     0.000000     0.0  \n",
       "50%       0.000000     0.00000     0.00000     0.000000     0.000000     0.0  \n",
       "75%       0.000000     0.00000     0.00000     1.000000     0.000000     0.0  \n",
       "max       1.000000     1.00000     1.00000     1.000000     1.000000     0.0  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "binary_columns = [ 'holiday', 'workingday','Feb', 'Mar', 'Apr',\n",
    "       'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', 'spring',\n",
    "       'summer', 'fall', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat',\n",
    "       'Cloudy', 'Snow', 'Storm']\n",
    "\n",
    "def standardize(train_df,test_df,cols_to_scale):\n",
    "    \n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    scaler=StandardScaler().fit(train[cols_to_scale])\n",
    "    train[cols_to_scale]=scaler.transform(train[cols_to_scale])\n",
    "    test[cols_to_scale]=scaler.transform(test[cols_to_scale])\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "to_scale=practice_X_train.columns.difference(binary_columns)\n",
    "\n",
    "X_scaled,_=standardize(practice_X_train,practice_X_train,to_scale)\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEBl-5bU48ci"
   },
   "source": [
    "**1.4** Write a code to augment your a dataset with higher-order features for `temp`, `atemp`, `hum`,`windspeed`, and `hour`. You should include ONLY pure powers of these columns. So with degree=2 you should produce `atemp^2` and `hum^2` but not `atemp*hum` or any other two-feature interactions. \n",
    "\n",
    "\n",
    "Encapsulate this process as a function with apropriate inputs and outputs, and test your code by producing `practice_X_train_poly`, a training dataset with qudratic and cubic features built from `practice_X_train_scaled`, and printing `practice_X_train_poly`'s column names and `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "UXgj9yxu48cj",
    "outputId": "d1e9e4f7-41ab-4cae-a900-e11e0890638e"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK5Mhlqc48dG"
   },
   "source": [
    "**1.5** Write code to add interaction terms to the model. Specifically, we want interactions between the continuous predictors (`temp`,`atemp`, `hum`,`windspeed`) and the month and weekday dummies (`Feb`, `Mar`...`Dec`, `Mon`, `Tue`, ... `Sat`). That means you SHOULD build `atemp*Feb` and `hum*Mon` and so on, but NOT `Feb*Mar` and NOT `Feb*Tue`. The interaction terms should always be a continuous feature times a month dummy or a continuous feature times a weekday dummy.\n",
    "\n",
    "\n",
    "Encapsulate this process as a function with appropriate inputs and outputs, and test your code by adding interaction terms to `practice_X_train_poly` and show its column names and `.head()`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gB6wlgj948dP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGd5qdPu48da"
   },
   "source": [
    "**1.6** Combine all your code so far into a function that takes in `bikes_train`, `bikes_val`, the names of columns for polynomial, the target column, the columns to be dropped and produces computation-ready design matrices `X_train` and `X_val` and responses `y_train` and `y_val`. Your final function should build correct, scaled design matrices with the stated interaction terms and any polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_design_mats(train_df, val_df,  degree, \n",
    "                    columns_forpoly=['temp', 'atemp', 'hum','windspeed', 'hour'],\n",
    "                    target_col='counts', \n",
    "                    bad_columns=['counts', 'registered', 'casual', 'workingday', 'month', 'dteday']):\n",
    "    # add code here \n",
    "    return x_train,y_train, x_val,y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 2 [20pts]: Regularization via Ridge </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** For each degree in 1 through 8:\n",
    "\n",
    "1.  Build the training design matrix and validation design matrix using the function `get_design_mats` with polynomial terms up through the specified degree.\n",
    "\n",
    "2.  Fit a regression model to the training data.\n",
    "\n",
    "3.  Report the model's score on the validation data.\n",
    "\n",
    "**2.2** Discuss patterns you see in the results from 2.1. Which model would you select, and why?\n",
    "\n",
    "**2.3** Let's try regularizing our models via ridge regression. Build a table showing the validation set $R^2$ of polynomial models with degree from 1-8, regularized at the levels $\\lambda = (.01, .05, .1,.5, 1, 5, 10, 50, 100)$. Do not perform cross validation at this point, simply report performance on the single validation set. \n",
    "\n",
    "**2.4** Find the best-scoring degree and regularization combination.\n",
    "\n",
    "**2.5** It's time to see how well our selected model will do on future data. Read in the provided test dataset, do any required formatting, and report the best model's $R^2$ score. How does it compare to the validation set score that made us choose this model? \n",
    "\n",
    "**2.6** Why do you think our model's test score was quite a bit worse than its validation score? Does the test set simply contain harder examples, or is something else going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** For each degree in 1 through 8:\n",
    "\n",
    "1.  Build the training design matrix and validation design matrix using the function `get_design_mats` with polynomial terms up through the specified degree.\n",
    "\n",
    "2.  Fit a regression model to the training data.\n",
    "\n",
    "3.  Report the model's score on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "VTWx4demHUo0",
    "outputId": "fb9ca7f7-8b7b-42e5-fab8-471ab5f57180"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Discuss patterns you see in the results from 2.1. Which model would you select, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQJ1vesGNwOf"
   },
   "source": [
    "**2.3** Let's try regularizing our models via ridge regression. Build a table showing the validation set $R^2$ of polynomial models with degree from 1-8, regularized at the levels $\\lambda = (.01, .05, .1,.5, 1, 5, 10, 50, 100)$. Do not perform cross validation at this point, simply report performance on the single validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "GUvYAWHzQAmb",
    "outputId": "7a60ee39-ee8e-46a9-aff7-781580b83717"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBwFV1wCTWnw"
   },
   "source": [
    "**2.4** Find the best-scoring degree and regularization combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "1zminoljTm9y",
    "outputId": "b12035c4-5a20-4449-fed4-dcc5e81c0db9"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KU3rqk4qU5BK"
   },
   "source": [
    "**2.5** It's time to see how well our selected model will do on future data. Read in the provided test dataset `data/bikes_test.csv`, do any required formatting, and report the best model's $R^2$ score. How does it compare to the validation set score that made us choose this model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbzfV7HaW_U1"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFjeBDXIW7nE"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Why do you think our model's test score was quite a bit worse than its validation score? Does the test set simply contain harder examples, or is something else going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 3 [20pts]: Comparing Ridge, Lasso, and OLS </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Build a dataset with polynomial degree 1 and fit an OLS model, a Ridge model, and a Lasso model. Use `RidgeCV` and `LassoCV` to select the best regularization level from among `(.1,.5,1,5,10,50,100)`. \n",
    "\n",
    "Note: On the lasso model, you will need to increase `max_iter` to 100,000 for the optimization to converge.\n",
    "\n",
    "**3.2** Plot histograms of the coefficients found by each of OLS, ridge, and lasso. What trends do you see in the magnitude of the coefficients?\n",
    "\n",
    "**3.3** The plots above show the overall distribution of coefficient values in each model, but do not show how each model treats individual coefficients. Build a plot which cleanly presents, for each feature in the data, 1) The coefficient assigned by OLS, 2) the coefficient assigned by ridge, and 3) the coefficient assigned by lasso.\n",
    "\n",
    "**Hint: Bar plots are a possible choice, but you are not required to use them**\n",
    "\n",
    "**Hint: use `xticks` to label coefficients with their feature names**\n",
    "\n",
    "**3.4** What trends do you see in the plot above? How do the three approaches handle the correlated pair `temp` and `atemp`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Build a dataset with polynomial degree 1 and fit an OLS model, a Ridge model, and a Lasso model. Use `RidgeCV` and `LassoCV` to select the best regularization level from among `(.1,.5,1,5,10,50,100)`. \n",
    "\n",
    "Note: On the lasso model, you will need to increase `max_iter` to 100,000 for the optimization to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3.2** Plot histograms of the coefficients found by each of OLS, ridge, and lasso. What trends do you see in the magnitude of the coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** The plots above show the overall distribution of coefficient values in each model, but do not show how each model treats individual coefficients. Build a plot which cleanly presents, for each feature in the data, 1) The coefficient assigned by OLS, 2) the coefficient assigned by ridge, and 3) the coefficient assigned by lasso.\n",
    "\n",
    "**Hint: Bar plots are a possible choice, but you are not required to use them**\n",
    "\n",
    "**Hint: use `xticks` to label coefficients with their feature names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** What trends do you see in the plot above? How do the three approaches handle the correlated pair `temp` and `atemp`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 4 [20 pts]: Reflection </b></div>\n",
    "These problems are open-ended, and you are not expected to write more than 2-3 sentences. We are interested in seeing that you have thought about these issues; you will be graded on how well you justify your conclusions here, not on what you conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Reflect back on the `get_design_mats` function you built. Writing this function useful in your analysis? What issues might you have encountered if you copy/pasted the model-building code instead of tying it together in a function? Does a `get_design_mat` function seem wise in general, or are there better options?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** What are the costs and benefits of applying ridge/lasso regularization to an overfit OLS model, versus setting a specific degree of polynomial or forward selecting features for the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4.3** This pset posed a purely predictive goal: forecast ridership as accurately as possible. How important is interpretability in this context? Considering, e.g., your lasso and ridge models from Question 3, how would you react if the models predicted well, but the coefficient values didn't make sense once interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**4.4** Reflect back on our original goal of helping BikeShare predict what demand will be like in the week ahead, and thus how many bikes they can bring in for maintenance. In your view, did we accomplish this goal? If yes, which model would you put into production and why? If not, which model came closest, what other analyses might you conduct, and how likely do you think they are to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "day_level_test.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
